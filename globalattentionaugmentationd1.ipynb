{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense,GlobalMaxPooling2D,Add,Reshape\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.backend import function\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.layers import Input, Multiply\nfrom tensorflow.keras.applications import InceptionV3,MobileNet,DenseNet201","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-02T03:42:22.347102Z","iopub.execute_input":"2023-10-02T03:42:22.347431Z","iopub.status.idle":"2023-10-02T03:42:22.353789Z","shell.execute_reply.started":"2023-10-02T03:42:22.347404Z","shell.execute_reply":"2023-10-02T03:42:22.352918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set a random seed for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T02:53:17.100144Z","iopub.status.idle":"2023-10-02T02:53:17.100868Z","shell.execute_reply.started":"2023-10-02T02:53:17.100621Z","shell.execute_reply":"2023-10-02T02:53:17.100651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Permute, Concatenate, Conv2D, Add, Activation, Lambda\nfrom keras import backend as K\nfrom keras.activations import sigmoid\ndef channel_attention(input_feature, ratio=8):\n\t\n\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n\tchannel = input_feature.shape[channel_axis]\n\t\n\tshared_layer_one = Dense(channel//ratio,\n\t\t\t\t\t\t\t activation='relu',\n\t\t\t\t\t\t\t kernel_initializer='he_normal',\n\t\t\t\t\t\t\t use_bias=True,\n\t\t\t\t\t\t\t bias_initializer='zeros')\n\tshared_layer_two = Dense(channel,\n\t\t\t\t\t\t\t kernel_initializer='he_normal',\n\t\t\t\t\t\t\t use_bias=True,\n\t\t\t\t\t\t\t bias_initializer='zeros')\n\t\n\tavg_pool = GlobalAveragePooling2D()(input_feature)    \n\tavg_pool = Reshape((1,1,channel))(avg_pool)\n\tassert avg_pool.shape[1:] == (1,1,channel)\n\tavg_pool = shared_layer_one(avg_pool)\n\tassert avg_pool.shape[1:] == (1,1,channel//ratio)\n\tavg_pool = shared_layer_two(avg_pool)\n\tassert avg_pool.shape[1:] == (1,1,channel)\n\t\n\tmax_pool = GlobalMaxPooling2D()(input_feature)\n\tmax_pool = Reshape((1,1,channel))(max_pool)\n\tassert max_pool.shape[1:] == (1,1,channel)\n\tmax_pool = shared_layer_one(max_pool)\n\tassert max_pool.shape[1:] == (1,1,channel//ratio)\n\tmax_pool = shared_layer_two(max_pool)\n\tassert max_pool.shape[1:] == (1,1,channel)\n\t\n\tcbam_feature = Add()([avg_pool,max_pool])\n\tcbam_feature = Activation('sigmoid')(cbam_feature)\n\t\n\tif K.image_data_format() == \"channels_first\":\n\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n\t\n\treturn multiply([input_feature, cbam_feature])","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:01:10.712750Z","iopub.execute_input":"2023-10-02T03:01:10.713099Z","iopub.status.idle":"2023-10-02T03:01:10.722007Z","shell.execute_reply.started":"2023-10-02T03:01:10.713070Z","shell.execute_reply":"2023-10-02T03:01:10.720862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def spatial_attention(input_feature):\n\tkernel_size = 7\n\t\n\tif K.image_data_format() == \"channels_first\":\n\t\tchannel = input_feature.shape[1]\n\t\tcbam_feature = Permute((2,3,1))(input_feature)\n\telse:\n\t\tchannel = input_feature.shape[-1]\n\t\tcbam_feature = input_feature\n\t\n\tavg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n\tassert avg_pool.shape[-1] == 1\n\tmax_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n\tassert max_pool.shape[-1] == 1\n\tconcat = Concatenate(axis=3)([avg_pool, max_pool])\n\tassert concat.shape[-1] == 2\n\tcbam_feature = Conv2D(filters = 1,\n\t\t\t\t\tkernel_size=kernel_size,\n\t\t\t\t\tstrides=1,\n\t\t\t\t\tpadding='same',\n\t\t\t\t\tactivation='sigmoid',\n\t\t\t\t\tkernel_initializer='he_normal',\n\t\t\t\t\tuse_bias=False)(concat)\t\n\tassert cbam_feature.shape[-1] == 1\n\t\n\tif K.image_data_format() == \"channels_first\":\n\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n\t\t\n\treturn multiply([input_feature, cbam_feature])","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:50:17.491813Z","iopub.execute_input":"2023-10-02T03:50:17.492494Z","iopub.status.idle":"2023-10-02T03:50:17.499546Z","shell.execute_reply.started":"2023-10-02T03:50:17.492463Z","shell.execute_reply":"2023-10-02T03:50:17.498624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cbam_block(cbam_feature, ratio=8):\n\t\"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n\tAs described in https://arxiv.org/abs/1807.06521.\n\t\"\"\"\n\t\n\tcbam_feature = channel_attention(cbam_feature, ratio)\n\tcbam_feature = spatial_attention(cbam_feature)\n\treturn cbam_feature\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:01:14.467180Z","iopub.execute_input":"2023-10-02T03:01:14.467494Z","iopub.status.idle":"2023-10-02T03:01:14.472407Z","shell.execute_reply.started":"2023-10-02T03:01:14.467468Z","shell.execute_reply":"2023-10-02T03:01:14.470954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def attention_module(input,ratio=16):\n    avg_pool = GlobalAveragePooling2D()(input)\n    max_pool = GlobalMaxPooling2D()(input)\n\n    channel_avg = Dense(units=input.shape[-1] // ratio, activation='relu')(avg_pool)\n    channel_max = Dense(units=input.shape[-1] // ratio, activation='relu')(max_pool)\n\n    channel_avg = Dense(units=input.shape[-1], activation='sigmoid')(channel_avg)\n    channel_max = Dense(units=input.shape[-1], activation='sigmoid')(channel_max)\n\n    channel_attention = Add()([channel_avg, channel_max])\n    channel_attention = Multiply()([input, Reshape((1, 1, input.shape[-1]))(channel_attention)])\n\n    return channel_attention\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T08:46:55.027063Z","iopub.execute_input":"2023-10-01T08:46:55.027396Z","iopub.status.idle":"2023-10-01T08:46:55.058924Z","shell.execute_reply.started":"2023-10-01T08:46:55.027367Z","shell.execute_reply":"2023-10-01T08:46:55.057985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_Global_attention_augmented_mobilenet(input_shape, num_classes):\n    # Load MobileNet base model without top layer\n    base_model = MobileNet(include_top=False, weights='imagenet', input_shape=input_shape)\n    base_model.trainable = False\n\n    # Attention augmented MobileNet architecture\n    input_tensor = Input(shape=input_shape)\n    x = base_model(input_tensor)\n\n    # Apply attention module\n    x = channel_attention(x,16)\n\n    # Add classification layers\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    output_tensor = Dense(num_classes, activation='softmax')(x)\n\n    # Create the model\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-10-02T02:58:22.468865Z","iopub.execute_input":"2023-10-02T02:58:22.469507Z","iopub.status.idle":"2023-10-02T02:58:22.475453Z","shell.execute_reply.started":"2023-10-02T02:58:22.469476Z","shell.execute_reply":"2023-10-02T02:58:22.473966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_Global_InceptionV3_model(input_shape, num_classes):\n    \n    # Load Inception-v3 base model without top layer\n    base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=input_shape)\n    base_model.trainable = False\n\n    # Attention augmented Inception-v3 architecture\n    input_tensor = Input(shape=input_shape)\n    x = base_model(input_tensor)\n\n    # Apply attention module\n    x = attention_module(x)\n\n    # Add classification layers\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    output_tensor = Dense(num_classes, activation='softmax')(x)\n\n    # Create the model\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T02:58:12.274719Z","iopub.execute_input":"2023-10-02T02:58:12.275058Z","iopub.status.idle":"2023-10-02T02:58:12.280345Z","shell.execute_reply.started":"2023-10-02T02:58:12.275030Z","shell.execute_reply":"2023-10-02T02:58:12.279321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import classification_report, balanced_accuracy_score, average_precision_score, confusion_matrix\n\ndef evaluate_classification(y_true, y_pred, average='macro'):\n    \"\"\"\n    Evaluate the classification performance and calculate micro-average, balanced accuracy, and average precision.\n\n    Parameters:\n        y_true (numpy array or list): True labels.\n        y_pred (numpy array or list): Predicted labels.\n        average (str, optional): The averaging strategy to use for average precision.\n                                 Possible values are 'macro', 'micro', 'weighted', and None.\n                                 Default is 'macro'.\n\n    Returns:\n        report (str): The classification report as a string.\n        balanced_acc (float): The balanced accuracy.\n        avg_precision (float): The average precision.\n        micro_avg_precision (float): The micro-average precision.\n        micro_avg_recall (float): The micro-average recall.\n        micro_avg_f1_score (float): The micro-average F1-score.\n    \"\"\"\n    report = classification_report(y_true, y_pred, output_dict=True, zero_division=1)\n    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n    avg_precision = average_precision_score(y_true, y_pred, average=average)\n\n    # Calculate micro-average precision and recall using confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    tp_sum = np.sum(np.diag(cm))\n    pred_sum = np.sum(cm, axis=0)\n    true_sum = np.sum(cm, axis=1)\n    micro_avg_precision = tp_sum / pred_sum.sum()\n    micro_avg_recall = tp_sum / true_sum.sum()\n    micro_avg_f1_score = 2 * (micro_avg_precision * micro_avg_recall) / (micro_avg_precision + micro_avg_recall)\n\n    return report, balanced_acc, avg_precision, micro_avg_precision, micro_avg_recall, micro_avg_f1_score\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T08:46:55.084669Z","iopub.execute_input":"2023-10-01T08:46:55.085514Z","iopub.status.idle":"2023-10-01T08:46:55.095638Z","shell.execute_reply.started":"2023-10-01T08:46:55.085478Z","shell.execute_reply":"2023-10-01T08:46:55.094761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. Load and split the dataset\ntrain_data_dir = '/kaggle/input/eye-diseases-classification/dataset'\n#validation_data_dir = 'd:/chaman/cataract/test'\ninput_shape = (224, 224)\nbatch_size = 32\nnum_classes=4\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T02:58:33.447520Z","iopub.execute_input":"2023-10-02T02:58:33.447843Z","iopub.status.idle":"2023-10-02T02:58:33.452181Z","shell.execute_reply.started":"2023-10-02T02:58:33.447816Z","shell.execute_reply":"2023-10-02T02:58:33.451210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2. Preprocess the images\ntrain_datagen = ImageDataGenerator(\n    rescale=1.0/255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2  # 20% validation split\n)\n\n#validation_datagen = ImageDataGenerator(rescale=1.0/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=input_shape,\n    batch_size=batch_size,\n    class_mode='categorical',  # Updated to 'categorical'\n    subset=\"training\"\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=input_shape,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'  # Subset for validation data\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T02:58:35.464932Z","iopub.execute_input":"2023-10-02T02:58:35.465252Z","iopub.status.idle":"2023-10-02T02:58:37.622966Z","shell.execute_reply.started":"2023-10-02T02:58:35.465228Z","shell.execute_reply":"2023-10-02T02:58:37.622117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nMobileNetModel = create_Global_attention_augmented_mobilenet(input_shape + (3,), num_classes)\n\n# 4. Compile the model\nMobileNetModel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:01:23.382339Z","iopub.execute_input":"2023-10-02T03:01:23.382666Z","iopub.status.idle":"2023-10-02T03:01:24.342290Z","shell.execute_reply.started":"2023-10-02T03:01:23.382638Z","shell.execute_reply":"2023-10-02T03:01:24.341260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InceptionV3Model = create_Global_InceptionV3_model(input_shape + (3,), num_classes)\n\nInceptionV3Model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T08:47:01.274694Z","iopub.execute_input":"2023-10-01T08:47:01.275033Z","iopub.status.idle":"2023-10-01T08:47:04.564729Z","shell.execute_reply.started":"2023-10-01T08:47:01.274988Z","shell.execute_reply":"2023-10-01T08:47:04.563780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:01:33.892244Z","iopub.execute_input":"2023-10-02T03:01:33.892560Z","iopub.status.idle":"2023-10-02T03:01:33.897833Z","shell.execute_reply.started":"2023-10-02T03:01:33.892534Z","shell.execute_reply":"2023-10-02T03:01:33.896502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5. Train the model\nepochs = 100\n\n# Create a directory to save the best model weights during training\nos.makedirs('models', exist_ok=True)\nmodel_checkpoint = ModelCheckpoint('models/best_GlobalmobilnetmodelD1.h5', save_best_only=True, save_weights_only=True)\n\nhistory = MobileNetModel.fit(\n    train_generator,\n    #steps_per_epoch=train_generator.n // train_generator.batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    #validation_steps=validation_generator.n // validation_generator.batch_size,\n    callbacks=[model_checkpoint,early_stopping]\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:01:37.701723Z","iopub.execute_input":"2023-10-02T03:01:37.702056Z","iopub.status.idle":"2023-10-02T03:41:08.254966Z","shell.execute_reply.started":"2023-10-02T03:01:37.702028Z","shell.execute_reply":"2023-10-02T03:41:08.253957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a directory to save the best model weights during training\nos.makedirs('models', exist_ok=True)\nmodel_checkpoint = ModelCheckpoint('models/best_GlobalInceptionModelD1.h5', save_best_only=True, save_weights_only=True)\n\nInceptionV3history = InceptionV3Model.fit(\n    train_generator,\n    #steps_per_epoch=train_generator.n // train_generator.batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    #validation_steps=validation_generator.n // validation_generator.batch_size,\n    callbacks=[model_checkpoint]\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T09:31:53.718906Z","iopub.execute_input":"2023-10-01T09:31:53.719925Z","iopub.status.idle":"2023-10-01T10:23:46.315737Z","shell.execute_reply.started":"2023-10-01T09:31:53.719891Z","shell.execute_reply":"2023-10-01T10:23:46.312601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MobileNetModel.save('GAAMD1.h5')\nInceptionV3Model.save('GAAIV3D1.h5')","metadata":{"execution":{"iopub.status.busy":"2023-10-01T10:23:46.316742Z","iopub.status.idle":"2023-10-01T10:23:46.317683Z","shell.execute_reply.started":"2023-10-01T10:23:46.317448Z","shell.execute_reply":"2023-10-01T10:23:46.317473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6. Evaluate the model\nMobileNetEvaluation = MobileNetModel.evaluate(validation_generator)\nprint(\"Validation Accuracy: {:.2f}%\".format(MobileNetEvaluation[1] * 100))","metadata":{"execution":{"iopub.status.busy":"2023-10-01T10:23:46.322386Z","iopub.status.idle":"2023-10-01T10:23:46.322774Z","shell.execute_reply.started":"2023-10-01T10:23:46.322585Z","shell.execute_reply":"2023-10-01T10:23:46.322605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6. Evaluate the model\nInceptionV3Evaluation = InceptionV3Model.evaluate(validation_generator)\nprint(\"Validation Accuracy: {:.2f}%\".format(InceptionV3Evaluation[1] * 100))","metadata":{"execution":{"iopub.status.busy":"2023-10-01T10:23:46.327651Z","iopub.status.idle":"2023-10-01T10:23:46.329757Z","shell.execute_reply.started":"2023-10-01T10:23:46.329543Z","shell.execute_reply":"2023-10-01T10:23:46.329566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_Global_attention_augmented_Densenet201_model(input_shape, num_classes):\n    \n    # Load Inception-v3 base model without top layer\n    base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=input_shape)\n    base_model.trainable = False\n\n    # Attention augmented Inception-v3 architecture\n    input_tensor = Input(shape=input_shape)\n    x = base_model(input_tensor)\n\n    # Apply attention module\n    x = cbam_block(x)\n\n    # Add classification layers\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    output_tensor = Dense(num_classes, activation='softmax')(x)\n\n    # Create the model\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:48:47.211119Z","iopub.execute_input":"2023-10-02T03:48:47.211436Z","iopub.status.idle":"2023-10-02T03:48:47.217357Z","shell.execute_reply.started":"2023-10-02T03:48:47.211410Z","shell.execute_reply":"2023-10-02T03:48:47.216452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nDenseNetModel = create_Global_attention_augmented_Densenet201_model(input_shape + (3,), num_classes)\n\n# 4. Compile the model\nDenseNetModel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:50:31.279280Z","iopub.execute_input":"2023-10-02T03:50:31.279616Z","iopub.status.idle":"2023-10-02T03:50:37.526058Z","shell.execute_reply.started":"2023-10-02T03:50:31.279589Z","shell.execute_reply":"2023-10-02T03:50:37.525135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5. Train the model\nepochs = 100\n\n# Create a directory to save the best model weights during training\nos.makedirs('models', exist_ok=True)\nmodel_checkpoint = ModelCheckpoint('models/best_GlobalDenseNetmodelD1.h5', save_best_only=True, save_weights_only=True)\n\nhistory = MobileNetModel.fit(\n    train_generator,\n    #steps_per_epoch=train_generator.n // train_generator.batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    #validation_steps=validation_generator.n // validation_generator.batch_size,\n    callbacks=[model_checkpoint,early_stopping]\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:51:01.841382Z","iopub.execute_input":"2023-10-02T03:51:01.841712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}