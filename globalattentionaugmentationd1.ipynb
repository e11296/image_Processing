{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense,GlobalMaxPooling2D,Add,Reshape\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.backend import function\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.layers import Input, Multiply\nfrom tensorflow.keras.applications import InceptionV3,MobileNet,DenseNet201","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-02T09:41:53.691597Z","iopub.execute_input":"2023-10-02T09:41:53.691986Z","iopub.status.idle":"2023-10-02T09:41:53.698984Z","shell.execute_reply.started":"2023-10-02T09:41:53.691957Z","shell.execute_reply":"2023-10-02T09:41:53.697756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set a random seed for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:41:55.199313Z","iopub.execute_input":"2023-10-02T09:41:55.199668Z","iopub.status.idle":"2023-10-02T09:41:55.266196Z","shell.execute_reply.started":"2023-10-02T09:41:55.199637Z","shell.execute_reply":"2023-10-02T09:41:55.265172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def attention_module(input,ratio=8):\n    avg_pool = GlobalAveragePooling2D()(input)\n    max_pool = GlobalMaxPooling2D()(input)\n\n    channel_avg = Dense(units=input.shape[-1] // ratio, activation='relu',kernel_initializer='he_normal')(avg_pool)\n    channel_max = Dense(units=input.shape[-1] // ratio, activation='relu',kernel_initializer='he_normal')(max_pool)\n\n    channel_avg = Dense(units=input.shape[-1], activation='sigmoid')(channel_avg)\n    channel_max = Dense(units=input.shape[-1], activation='sigmoid')(channel_max)\n\n    channel_attention = Add()([channel_avg, channel_max])\n    channel_attention = Multiply()([input, Reshape((1, 1, input.shape[-1]))(channel_attention)])\n\n    return channel_attention\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:42:29.417198Z","iopub.execute_input":"2023-10-02T09:42:29.417579Z","iopub.status.idle":"2023-10-02T09:42:29.424580Z","shell.execute_reply.started":"2023-10-02T09:42:29.417548Z","shell.execute_reply":"2023-10-02T09:42:29.423565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_Global_attention_augmented_mobilenet(input_shape, num_classes,attention=False):\n    # Load MobileNet base model without top layer\n    base_model = MobileNet(include_top=False, weights='imagenet', input_shape=input_shape)\n    base_model.trainable = False\n\n    # Attention augmented MobileNet architecture\n    input_tensor = Input(shape=input_shape)\n    x = base_model(input_tensor)\n\n    # Apply attention module\n    if(attention):\n        x = attention_module(x)\n\n    # Add classification layers\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    output_tensor = Dense(num_classes, activation='softmax')(x)\n\n    # Create the model\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:42:40.588299Z","iopub.execute_input":"2023-10-02T09:42:40.588644Z","iopub.status.idle":"2023-10-02T09:42:40.594266Z","shell.execute_reply.started":"2023-10-02T09:42:40.588616Z","shell.execute_reply":"2023-10-02T09:42:40.593238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_Global_InceptionV3_model(input_shape, num_classes,attention=False):\n    \n    # Load Inception-v3 base model without top layer\n    base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=input_shape)\n    base_model.trainable = False\n\n    # Attention augmented Inception-v3 architecture\n    input_tensor = Input(shape=input_shape)\n    x = base_model(input_tensor)\n\n    # Apply attention module\n    if(attention):\n        x = attention_module(x)\n\n    # Add classification layers\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    output_tensor = Dense(num_classes, activation='softmax')(x)\n\n    # Create the model\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:43:01.747632Z","iopub.execute_input":"2023-10-02T09:43:01.747997Z","iopub.status.idle":"2023-10-02T09:43:01.753944Z","shell.execute_reply.started":"2023-10-02T09:43:01.747968Z","shell.execute_reply":"2023-10-02T09:43:01.752982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import classification_report, balanced_accuracy_score, average_precision_score, confusion_matrix\n\ndef evaluate_classification(y_true, y_pred, average='macro'):\n    \"\"\"\n    Evaluate the classification performance and calculate micro-average, balanced accuracy, and average precision.\n\n    Parameters:\n        y_true (numpy array or list): True labels.\n        y_pred (numpy array or list): Predicted labels.\n        average (str, optional): The averaging strategy to use for average precision.\n                                 Possible values are 'macro', 'micro', 'weighted', and None.\n                                 Default is 'macro'.\n\n    Returns:\n        report (str): The classification report as a string.\n        balanced_acc (float): The balanced accuracy.\n        avg_precision (float): The average precision.\n        micro_avg_precision (float): The micro-average precision.\n        micro_avg_recall (float): The micro-average recall.\n        micro_avg_f1_score (float): The micro-average F1-score.\n    \"\"\"\n    report = classification_report(y_true, y_pred, output_dict=True, zero_division=1)\n    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n    avg_precision = average_precision_score(y_true, y_pred, average=average)\n\n    # Calculate micro-average precision and recall using confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    tp_sum = np.sum(np.diag(cm))\n    pred_sum = np.sum(cm, axis=0)\n    true_sum = np.sum(cm, axis=1)\n    micro_avg_precision = tp_sum / pred_sum.sum()\n    micro_avg_recall = tp_sum / true_sum.sum()\n    micro_avg_f1_score = 2 * (micro_avg_precision * micro_avg_recall) / (micro_avg_precision + micro_avg_recall)\n\n    return report, balanced_acc, avg_precision, micro_avg_precision, micro_avg_recall, micro_avg_f1_score\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:43:06.438849Z","iopub.execute_input":"2023-10-02T09:43:06.439200Z","iopub.status.idle":"2023-10-02T09:43:06.446725Z","shell.execute_reply.started":"2023-10-02T09:43:06.439174Z","shell.execute_reply":"2023-10-02T09:43:06.445687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. Load and split the dataset\ntrain_data_dir = '/kaggle/input/eye-diseases-classification/dataset'\n#validation_data_dir = 'd:/chaman/cataract/test'\ninput_shape = (224, 224)\nbatch_size = 32\nnum_classes=4\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:43:07.418712Z","iopub.execute_input":"2023-10-02T09:43:07.419097Z","iopub.status.idle":"2023-10-02T09:43:07.424005Z","shell.execute_reply.started":"2023-10-02T09:43:07.419070Z","shell.execute_reply":"2023-10-02T09:43:07.422782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2. Preprocess the images\ntrain_datagen = ImageDataGenerator(\n    rescale=1.0/255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2,  # 20% validation split\n)\n\n#validation_datagen = ImageDataGenerator(rescale=1.0/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=input_shape,\n    batch_size=batch_size,\n    class_mode='categorical',  # Updated to 'categorical'\n    subset=\"training\"\n)\n\n\nvalidation_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=input_shape,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'  # Subset for validation data\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:54:36.388918Z","iopub.execute_input":"2023-10-02T09:54:36.389254Z","iopub.status.idle":"2023-10-02T09:54:36.610353Z","shell.execute_reply.started":"2023-10-02T09:54:36.389229Z","shell.execute_reply":"2023-10-02T09:54:36.609449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_counts = train_generator.classes\nclass_indices = train_generator.class_indices\nnum_classes = len(class_indices)\n\n# Create a dictionary to store the counts for each class\nclass_counts_dict = {class_name: np.sum(class_counts == class_idx) for class_name, class_idx in class_indices.items()}\n\n# Print the class counts\nfor class_name, count in class_counts_dict.items():\n    print(f\"Class '{class_name}': {count} samples\")\n\n# Alternatively, you can simply print the 'class_counts_dict' dictionary\nprint(class_counts_dict)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:54:37.211282Z","iopub.execute_input":"2023-10-02T09:54:37.212179Z","iopub.status.idle":"2023-10-02T09:54:37.218962Z","shell.execute_reply.started":"2023-10-02T09:54:37.212145Z","shell.execute_reply":"2023-10-02T09:54:37.218031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nMobileNetModel = create_Global_attention_augmented_mobilenet(input_shape + (3,), num_classes,attention=True)\n\n# 4. Compile the model\nMobileNetModel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:54:40.727149Z","iopub.execute_input":"2023-10-02T09:54:40.727488Z","iopub.status.idle":"2023-10-02T09:54:41.608821Z","shell.execute_reply.started":"2023-10-02T09:54:40.727461Z","shell.execute_reply":"2023-10-02T09:54:41.607862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InceptionV3Model = create_Global_InceptionV3_model(input_shape + (3,), num_classes,attention=True)\n\nInceptionV3Model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:54:41.610725Z","iopub.execute_input":"2023-10-02T09:54:41.611134Z","iopub.status.idle":"2023-10-02T09:54:44.371994Z","shell.execute_reply.started":"2023-10-02T09:54:41.611098Z","shell.execute_reply":"2023-10-02T09:54:44.371037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=40, verbose=0, restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:54:44.373764Z","iopub.execute_input":"2023-10-02T09:54:44.374342Z","iopub.status.idle":"2023-10-02T09:54:44.379574Z","shell.execute_reply.started":"2023-10-02T09:54:44.374307Z","shell.execute_reply":"2023-10-02T09:54:44.378573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5. Train the model\nepochs = 100\n\n# Create a directory to save the best model weights during training\nos.makedirs('models', exist_ok=True)\nmodel_checkpoint = ModelCheckpoint('models/best_GlobalmobilnetmodelD1.h5', save_best_only=True, save_weights_only=True)\n\nhistory = MobileNetModel.fit(\n    train_generator,\n    #steps_per_epoch=train_generator.n // train_generator.batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    #validation_steps=validation_generator.n // validation_generator.batch_size,\n    callbacks=[model_checkpoint,early_stopping]\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:54:44.380865Z","iopub.execute_input":"2023-10-02T09:54:44.381575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a directory to save the best model weights during training\nos.makedirs('models', exist_ok=True)\nmodel_checkpoint = ModelCheckpoint('models/best_GlobalInceptionModelD1.h5', save_best_only=True, save_weights_only=True)\n\nInceptionV3history = InceptionV3Model.fit(\n    train_generator,\n    #steps_per_epoch=train_generator.n // train_generator.batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    #validation_steps=validation_generator.n // validation_generator.batch_size,\n    callbacks=[model_checkpoint]\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MobileNetModel.save('GAAMD1.h5')\nInceptionV3Model.save('GAAIV3D1.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6. Evaluate the model\nMobileNetEvaluation = MobileNetModel.evaluate(validation_generator)\nprint(\"Validation Accuracy: {:.2f}%\".format(MobileNetEvaluation[1] * 100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6. Evaluate the model\nInceptionV3Evaluation = InceptionV3Model.evaluate(validation_generator)\nprint(\"Validation Accuracy: {:.2f}%\".format(InceptionV3Evaluation[1] * 100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_Global_attention_augmented_Densenet201_model(input_shape, num_classes,attention=False):\n    \n    # Load Inception-v3 base model without top layer\n    base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=input_shape)\n    base_model.trainable = False\n\n    # Attention augmented Inception-v3 architecture\n    input_tensor = Input(shape=input_shape)\n    x = base_model(input_tensor)\n\n    # Apply attention module\n    if(attention):\n        x=chanel_attention(x,ratio=16)\n    #x = channel_attention(x,16)\n\n    # Add classification layers\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    output_tensor = Dense(num_classes, activation='softmax')(x)\n\n    # Create the model\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nDenseNetModel = create_Global_attention_augmented_Densenet201_model(input_shape + (3,), num_classes,attention=True)\n\n# 4. Compile the model\nDenseNetModel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5. Train the model\nepochs = 100\n\n# Create a directory to save the best model weights during training\nos.makedirs('models', exist_ok=True)\nmodel_checkpoint = ModelCheckpoint('models/best_GlobalDenseNetmodelD1.h5', save_best_only=True, save_weights_only=True)\n\nhistory = DenseNetModel.fit(\n    train_generator,\n    #steps_per_epoch=train_generator.n // train_generator.batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    #validation_steps=validation_generator.n // validation_generator.batch_size,\n    callbacks=[model_checkpoint,early_stopping]\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}