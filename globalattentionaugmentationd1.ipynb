{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense,GlobalMaxPooling2D,Add,Reshape\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.backend import function\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.layers import Input, Multiply\nfrom tensorflow.keras.applications import InceptionV3,MobileNet,DenseNet201","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-02T07:30:32.271648Z","iopub.execute_input":"2023-10-02T07:30:32.272220Z","iopub.status.idle":"2023-10-02T07:30:35.388983Z","shell.execute_reply.started":"2023-10-02T07:30:32.272186Z","shell.execute_reply":"2023-10-02T07:30:35.387853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set a random seed for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:30:35.390573Z","iopub.execute_input":"2023-10-02T07:30:35.391675Z","iopub.status.idle":"2023-10-02T07:30:35.397610Z","shell.execute_reply.started":"2023-10-02T07:30:35.391631Z","shell.execute_reply":"2023-10-02T07:30:35.396323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Permute, Concatenate, Conv2D, Add, Activation, Lambda\nfrom keras import backend as K\nfrom keras.activations import sigmoid\ndef channel_attention(input_feature, ratio=8):\n\t\n\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n\tchannel = input_feature.shape[channel_axis]\n\t\n\tshared_layer_one = Dense(channel//ratio,\n\t\t\t\t\t\t\t activation='relu',\n\t\t\t\t\t\t\t kernel_initializer='he_normal',\n\t\t\t\t\t\t\t use_bias=True,\n\t\t\t\t\t\t\t bias_initializer='zeros')\n\tshared_layer_two = Dense(channel,\n\t\t\t\t\t\t\t kernel_initializer='he_normal',\n\t\t\t\t\t\t\t use_bias=True,\n\t\t\t\t\t\t\t bias_initializer='zeros')\n\t\n\tavg_pool = GlobalAveragePooling2D()(input_feature)    \n\tavg_pool = Reshape((1,1,channel))(avg_pool)\n\tassert avg_pool.shape[1:] == (1,1,channel)\n\tavg_pool = shared_layer_one(avg_pool)\n\tassert avg_pool.shape[1:] == (1,1,channel//ratio)\n\tavg_pool = shared_layer_two(avg_pool)\n\tassert avg_pool.shape[1:] == (1,1,channel)\n\t\n\tmax_pool = GlobalMaxPooling2D()(input_feature)\n\tmax_pool = Reshape((1,1,channel))(max_pool)\n\tassert max_pool.shape[1:] == (1,1,channel)\n\tmax_pool = shared_layer_one(max_pool)\n\tassert max_pool.shape[1:] == (1,1,channel//ratio)\n\tmax_pool = shared_layer_two(max_pool)\n\tassert max_pool.shape[1:] == (1,1,channel)\n\t\n\tcbam_feature = Add()([avg_pool,max_pool])\n\tcbam_feature = Activation('sigmoid')(cbam_feature)\n\t\n\tif K.image_data_format() == \"channels_first\":\n\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n\t\n\treturn multiply([input_feature, cbam_feature])","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:30:35.399067Z","iopub.execute_input":"2023-10-02T07:30:35.400210Z","iopub.status.idle":"2023-10-02T07:30:35.411422Z","shell.execute_reply.started":"2023-10-02T07:30:35.400175Z","shell.execute_reply":"2023-10-02T07:30:35.410288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def spatial_attention(input_feature):\n\tkernel_size = 7\n\t\n\tif K.image_data_format() == \"channels_first\":\n\t\tchannel = input_feature.shape[1]\n\t\tcbam_feature = Permute((2,3,1))(input_feature)\n\telse:\n\t\tchannel = input_feature.shape[-1]\n\t\tcbam_feature = input_feature\n\t\n\tavg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n\tassert avg_pool.shape[-1] == 1\n\tmax_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n\tassert max_pool.shape[-1] == 1\n\tconcat = Concatenate(axis=3)([avg_pool, max_pool])\n\tassert concat.shape[-1] == 2\n\tcbam_feature = Conv2D(filters = 1,\n\t\t\t\t\tkernel_size=kernel_size,\n\t\t\t\t\tstrides=1,\n\t\t\t\t\tpadding='same',\n\t\t\t\t\tactivation='sigmoid',\n\t\t\t\t\tkernel_initializer='he_normal',\n\t\t\t\t\tuse_bias=False)(concat)\t\n\tassert cbam_feature.shape[-1] == 1\n\t\n\tif K.image_data_format() == \"channels_first\":\n\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n\t\t\n\treturn multiply([input_feature, cbam_feature])","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:30:35.415184Z","iopub.execute_input":"2023-10-02T07:30:35.415945Z","iopub.status.idle":"2023-10-02T07:30:35.425857Z","shell.execute_reply.started":"2023-10-02T07:30:35.415864Z","shell.execute_reply":"2023-10-02T07:30:35.424926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cbam_block(cbam_feature, ratio=8):\n\t\"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n\tAs described in https://arxiv.org/abs/1807.06521.\n\t\"\"\"\n\t\n\tcbam_feature = channel_attention(cbam_feature, ratio)\n\tcbam_feature = spatial_attention(cbam_feature)\n\treturn cbam_feature\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:30:35.427188Z","iopub.execute_input":"2023-10-02T07:30:35.427727Z","iopub.status.idle":"2023-10-02T07:30:35.441818Z","shell.execute_reply.started":"2023-10-02T07:30:35.427694Z","shell.execute_reply":"2023-10-02T07:30:35.440859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def attention_module(input,ratio=16):\n    avg_pool = GlobalAveragePooling2D()(input)\n    max_pool = GlobalMaxPooling2D()(input)\n\n    channel_avg = Dense(units=input.shape[-1] // ratio, activation='relu')(avg_pool)\n    channel_max = Dense(units=input.shape[-1] // ratio, activation='relu')(max_pool)\n\n    channel_avg = Dense(units=input.shape[-1], activation='sigmoid')(channel_avg)\n    channel_max = Dense(units=input.shape[-1], activation='sigmoid')(channel_max)\n\n    channel_attention = Add()([channel_avg, channel_max])\n    channel_attention = Multiply()([input, Reshape((1, 1, input.shape[-1]))(channel_attention)])\n\n    return channel_attention\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:30:35.443173Z","iopub.execute_input":"2023-10-02T07:30:35.443846Z","iopub.status.idle":"2023-10-02T07:30:35.453913Z","shell.execute_reply.started":"2023-10-02T07:30:35.443770Z","shell.execute_reply":"2023-10-02T07:30:35.452844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_Global_attention_augmented_mobilenet(input_shape, num_classes):\n    # Load MobileNet base model without top layer\n    base_model = MobileNet(include_top=False, weights='imagenet', input_shape=input_shape)\n    base_model.trainable = False\n\n    # Attention augmented MobileNet architecture\n    input_tensor = Input(shape=input_shape)\n    x = base_model(input_tensor)\n\n    # Apply attention module\n    x = channel_attention(x,16)\n\n    # Add classification layers\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    output_tensor = Dense(num_classes, activation='softmax')(x)\n\n    # Create the model\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:30:35.455265Z","iopub.execute_input":"2023-10-02T07:30:35.456458Z","iopub.status.idle":"2023-10-02T07:30:35.466264Z","shell.execute_reply.started":"2023-10-02T07:30:35.456425Z","shell.execute_reply":"2023-10-02T07:30:35.465237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_Global_InceptionV3_model(input_shape, num_classes):\n    \n    # Load Inception-v3 base model without top layer\n    base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=input_shape)\n    base_model.trainable = False\n\n    # Attention augmented Inception-v3 architecture\n    input_tensor = Input(shape=input_shape)\n    x = base_model(input_tensor)\n\n    # Apply attention module\n    x = attention_module(x)\n\n    # Add classification layers\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    output_tensor = Dense(num_classes, activation='softmax')(x)\n\n    # Create the model\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:30:35.469282Z","iopub.execute_input":"2023-10-02T07:30:35.469556Z","iopub.status.idle":"2023-10-02T07:30:35.482127Z","shell.execute_reply.started":"2023-10-02T07:30:35.469533Z","shell.execute_reply":"2023-10-02T07:30:35.481057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import classification_report, balanced_accuracy_score, average_precision_score, confusion_matrix\n\ndef evaluate_classification(y_true, y_pred, average='macro'):\n    \"\"\"\n    Evaluate the classification performance and calculate micro-average, balanced accuracy, and average precision.\n\n    Parameters:\n        y_true (numpy array or list): True labels.\n        y_pred (numpy array or list): Predicted labels.\n        average (str, optional): The averaging strategy to use for average precision.\n                                 Possible values are 'macro', 'micro', 'weighted', and None.\n                                 Default is 'macro'.\n\n    Returns:\n        report (str): The classification report as a string.\n        balanced_acc (float): The balanced accuracy.\n        avg_precision (float): The average precision.\n        micro_avg_precision (float): The micro-average precision.\n        micro_avg_recall (float): The micro-average recall.\n        micro_avg_f1_score (float): The micro-average F1-score.\n    \"\"\"\n    report = classification_report(y_true, y_pred, output_dict=True, zero_division=1)\n    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n    avg_precision = average_precision_score(y_true, y_pred, average=average)\n\n    # Calculate micro-average precision and recall using confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    tp_sum = np.sum(np.diag(cm))\n    pred_sum = np.sum(cm, axis=0)\n    true_sum = np.sum(cm, axis=1)\n    micro_avg_precision = tp_sum / pred_sum.sum()\n    micro_avg_recall = tp_sum / true_sum.sum()\n    micro_avg_f1_score = 2 * (micro_avg_precision * micro_avg_recall) / (micro_avg_precision + micro_avg_recall)\n\n    return report, balanced_acc, avg_precision, micro_avg_precision, micro_avg_recall, micro_avg_f1_score\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:30:35.483841Z","iopub.execute_input":"2023-10-02T07:30:35.484460Z","iopub.status.idle":"2023-10-02T07:30:35.494760Z","shell.execute_reply.started":"2023-10-02T07:30:35.484427Z","shell.execute_reply":"2023-10-02T07:30:35.493791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. Load and split the dataset\ntrain_data_dir = '/kaggle/input/eye-diseases-classification/dataset'\n#validation_data_dir = 'd:/chaman/cataract/test'\ninput_shape = (224, 224)\nbatch_size = 32\nnum_classes=4\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:30:35.496347Z","iopub.execute_input":"2023-10-02T07:30:35.497072Z","iopub.status.idle":"2023-10-02T07:30:35.510288Z","shell.execute_reply.started":"2023-10-02T07:30:35.497039Z","shell.execute_reply":"2023-10-02T07:30:35.509158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2. Preprocess the images\ntrain_datagen = ImageDataGenerator(\n    rescale=1.0/255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.3,  # 20% validation split\n)\n\n#validation_datagen = ImageDataGenerator(rescale=1.0/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=input_shape,\n    batch_size=batch_size,\n    class_mode='categorical',  # Updated to 'categorical'\n    subset=\"training\"\n)\n\n\nvalidation_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=input_shape,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'  # Subset for validation data\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T08:06:18.468715Z","iopub.execute_input":"2023-10-02T08:06:18.469167Z","iopub.status.idle":"2023-10-02T08:06:19.068288Z","shell.execute_reply.started":"2023-10-02T08:06:18.469133Z","shell.execute_reply":"2023-10-02T08:06:19.067310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_counts = train_generator.classes\nclass_indices = train_generator.class_indices\nnum_classes = len(class_indices)\n\n# Create a dictionary to store the counts for each class\nclass_counts_dict = {class_name: np.sum(class_counts == class_idx) for class_name, class_idx in class_indices.items()}\n\n# Print the class counts\nfor class_name, count in class_counts_dict.items():\n    print(f\"Class '{class_name}': {count} samples\")\n\n# Alternatively, you can simply print the 'class_counts_dict' dictionary\nprint(class_counts_dict)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T08:06:22.436332Z","iopub.execute_input":"2023-10-02T08:06:22.437415Z","iopub.status.idle":"2023-10-02T08:06:22.445172Z","shell.execute_reply.started":"2023-10-02T08:06:22.437374Z","shell.execute_reply":"2023-10-02T08:06:22.443962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nMobileNetModel = create_Global_attention_augmented_mobilenet(input_shape + (3,), num_classes)\n\n# 4. Compile the model\nMobileNetModel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T08:06:26.739453Z","iopub.execute_input":"2023-10-02T08:06:26.740014Z","iopub.status.idle":"2023-10-02T08:06:27.640190Z","shell.execute_reply.started":"2023-10-02T08:06:26.739982Z","shell.execute_reply":"2023-10-02T08:06:27.639210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InceptionV3Model = create_Global_InceptionV3_model(input_shape + (3,), num_classes)\n\nInceptionV3Model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T08:06:28.706061Z","iopub.execute_input":"2023-10-02T08:06:28.706407Z","iopub.status.idle":"2023-10-02T08:06:31.933940Z","shell.execute_reply.started":"2023-10-02T08:06:28.706377Z","shell.execute_reply":"2023-10-02T08:06:31.932855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=40, verbose=0, restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T08:06:31.935516Z","iopub.execute_input":"2023-10-02T08:06:31.936150Z","iopub.status.idle":"2023-10-02T08:06:31.942000Z","shell.execute_reply.started":"2023-10-02T08:06:31.936115Z","shell.execute_reply":"2023-10-02T08:06:31.940963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5. Train the model\nepochs = 100\n\n# Create a directory to save the best model weights during training\nos.makedirs('models', exist_ok=True)\nmodel_checkpoint = ModelCheckpoint('models/best_GlobalmobilnetmodelD1.h5', save_best_only=True, save_weights_only=True)\n\nhistory = MobileNetModel.fit(\n    train_generator,\n    #steps_per_epoch=train_generator.n // train_generator.batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    #validation_steps=validation_generator.n // validation_generator.batch_size,\n    callbacks=[model_checkpoint,early_stopping]\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T08:06:32.933190Z","iopub.execute_input":"2023-10-02T08:06:32.933539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a directory to save the best model weights during training\nos.makedirs('models', exist_ok=True)\nmodel_checkpoint = ModelCheckpoint('models/best_GlobalInceptionModelD1.h5', save_best_only=True, save_weights_only=True)\n\nInceptionV3history = InceptionV3Model.fit(\n    train_generator,\n    #steps_per_epoch=train_generator.n // train_generator.batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    #validation_steps=validation_generator.n // validation_generator.batch_size,\n    callbacks=[model_checkpoint]\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MobileNetModel.save('GAAMD1.h5')\nInceptionV3Model.save('GAAIV3D1.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6. Evaluate the model\nMobileNetEvaluation = MobileNetModel.evaluate(validation_generator)\nprint(\"Validation Accuracy: {:.2f}%\".format(MobileNetEvaluation[1] * 100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6. Evaluate the model\nInceptionV3Evaluation = InceptionV3Model.evaluate(validation_generator)\nprint(\"Validation Accuracy: {:.2f}%\".format(InceptionV3Evaluation[1] * 100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_Global_attention_augmented_Densenet201_model(input_shape, num_classes):\n    \n    # Load Inception-v3 base model without top layer\n    base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=input_shape)\n    base_model.trainable = False\n\n    # Attention augmented Inception-v3 architecture\n    input_tensor = Input(shape=input_shape)\n    x = base_model(input_tensor)\n\n    # Apply attention module\n    #x = channel_attention(x,16)\n\n    # Add classification layers\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    output_tensor = Dense(num_classes, activation='softmax')(x)\n\n    # Create the model\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nDenseNetModel = create_Global_attention_augmented_Densenet201_model(input_shape + (3,), num_classes)\n\n# 4. Compile the model\nDenseNetModel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5. Train the model\nepochs = 100\n\n# Create a directory to save the best model weights during training\nos.makedirs('models', exist_ok=True)\nmodel_checkpoint = ModelCheckpoint('models/best_GlobalDenseNetmodelD1.h5', save_best_only=True, save_weights_only=True)\n\nhistory = DenseNetModel.fit(\n    train_generator,\n    #steps_per_epoch=train_generator.n // train_generator.batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    #validation_steps=validation_generator.n // validation_generator.batch_size,\n    callbacks=[model_checkpoint,early_stopping]\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}